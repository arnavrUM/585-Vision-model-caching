We propose \textbf{Multi-Level Caching (MLC)}, a hierarchical caching framework that targets redundancy at multiple abstraction levels and stages of MLLM inference.
MLC consists of three cache levels: (1) \textbf{L0.5: Exact Text Cache}, which performs fast normalized text matching to identify verbatim prompt duplicates; (2) \textbf{L1: Semantic Text Cache}, which uses sentence-transformers to detect semantically equivalent queries (e.g., paraphrases) via cosine similarity in a learned embedding space; and (3) \textbf{L2: Embedding Cache}, which operates on latent representations extracted from intermediate vision encoder and prompt encoder layers, enabling reuse when inputs are visually or semantically similar but not textually identical.

When any cache level identifies a match, MLC injects the corresponding cached key--value (KV) attention blocks directly into the inference engine's GPU memory, bypassing the forward pass through the vision encoder, multimodal fusion, and early transformer layers. This cross-request KV reuse complements vLLM's built-in prefix caching (which only operates within a single request) and can eliminate the majority of computation for repeated or similar queries. On cache misses, MLC captures and persists KV blocks along with their associated embeddings for future reuse, building a progressively more effective cache over time.

Our evaluation on video frame sequences and multi-turn visual question answering demonstrates that MLC achieves cache hit rates of 95--100\% on datasets with high redundancy, reducing end-to-end latency by up to 3$\times$ compared to baseline inference while maintaining answer accuracy. The hierarchical design allows the system to balance precision (via embedding-level matching) with speed (via exact text matching), making it practical for production deployment in interactive multimodal systems.

