[2025-12-11 06:37:11] INFO loader.py:120: Loading faiss with AVX512 support.
[2025-12-11 06:37:11] INFO loader.py:122: Successfully loaded faiss with AVX512 support.
[info] cache_dir resolved to /workspace/experiment2/kv_chunks
[info] fusion_cache_dir resolved to /workspace/experiment2/fusion_chunks
Loaded 64 prompts from Video Frames (video_frames_dataset.json).
Chunk-key uniqueness: 3 unique keys / 64 prompts (100.0% reused >=2x)
Most frequent chunk keys:
- occurrences=48: Is the cap on the bottle?
- occurrences=11: What is in this frame?
- occurrences=5: Is a hand visible in this frame?
INFO 12-11 06:37:11 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'max_model_len': 4096, 'model': 'OpenGVLab/InternVL3_5-4B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
WARNING 12-11 06:37:12 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
WARNING 12-11 06:37:12 [arg_utils.py:1183] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.
A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL3_5-4B-Instruct:
- configuration_intern_vit.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL3_5-4B-Instruct:
- configuration_internvl_chat.py
- configuration_intern_vit.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
[2025-12-11 06:37:14] INFO configuration_internvl_chat.py:87: vision_select_layer: -1
[2025-12-11 06:37:14] INFO configuration_internvl_chat.py:88: ps_version: v2
[2025-12-11 06:37:14] INFO configuration_internvl_chat.py:89: min_dynamic_patch: 1
[2025-12-11 06:37:14] INFO configuration_internvl_chat.py:90: max_dynamic_patch: 12
INFO 12-11 06:37:14 [model.py:637] Resolved architecture: InternVLChatModel
INFO 12-11 06:37:14 [model.py:1750] Using max model len 4096
INFO 12-11 06:37:14 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-11 06:37:19 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='OpenGVLab/InternVL3_5-4B-Instruct', speculative_config=None, tokenizer='OpenGVLab/InternVL3_5-4B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=OpenGVLab/InternVL3_5-4B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
INFO 12-11 06:37:20 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.16.96.2:53619 backend=nccl
INFO 12-11 06:37:20 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
WARNING 12-11 06:37:20 [func_utils.py:230] The following intended overrides are not keyword args and will be dropped: {'truncation'}
WARNING 12-11 06:37:20 [func_utils.py:230] The following intended overrides are not keyword args and will be dropped: {'truncation'}
INFO 12-11 06:37:20 [gpu_model_runner.py:3467] Starting to load model OpenGVLab/InternVL3_5-4B-Instruct...
INFO 12-11 06:37:20 [layer.py:500] Using AttentionBackendEnum.FLASH_ATTN for MultiHeadAttention in multimodal encoder.
INFO 12-11 06:37:21 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/585-Vision-model-caching/experiment2/test_vllm.py", line 1274, in <module>
[rank0]:     main()
[rank0]:   File "/root/585-Vision-model-caching/experiment2/test_vllm.py", line 1188, in main
[rank0]:     llm = LLM(
[rank0]:           ^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 334, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
[rank0]:     return cls(
[rank0]:            ^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py", line 109, in __init__
[rank0]:     self.engine_core = EngineCoreClient.make_client(
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 95, in make_client
[rank0]:     return InprocClient(vllm_config, executor_class, log_stats)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 266, in __init__
[rank0]:     self.engine_core = EngineCore(*args, **kwargs)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[rank0]:     self.model_executor = executor_class(vllm_config)
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[rank0]:     self.driver_worker.load_model()
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[rank0]:     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3484, in load_model
[rank0]:     self.model = model_loader.load_model(
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 55, in load_model
[rank0]:     self.load_weights(model, model_config)
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py", line 305, in load_weights
[rank0]:     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/models/internvl.py", line 1442, in load_weights
[rank0]:     return loader.load_weights(weights)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[rank0]:     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[rank0]:     autoloaded_weights = set(self._load_module("", self.module, weights))
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 279, in _load_module
[rank0]:     for child_prefix, child_weights in self._groupby_prefix(weights):
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[rank0]:     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 158, in <genexpr>
[rank0]:     weights_by_parts = (
[rank0]:                        ^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 331, in <genexpr>
[rank0]:     weights = (
[rank0]:               ^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py", line 277, in get_all_weights
[rank0]:     yield from self._get_weights_iterator(primary_weights)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py", line 190, in _get_weights_iterator
[rank0]:     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[rank0]:                                                    ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py", line 141, in _prepare_weights
[rank0]:     hf_folder = download_weights_from_hf(
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 472, in download_weights_from_hf
[rank0]:     hf_folder = snapshot_download(
[rank0]:                 ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py", line 332, in snapshot_download
[rank0]:     thread_map(
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/tqdm/contrib/concurrent.py", line 69, in thread_map
[rank0]:     return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/tqdm/contrib/concurrent.py", line 51, in _executor_map
[rank0]:     return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/tqdm/std.py", line 1169, in __iter__
[rank0]:     for obj in iterable:
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/concurrent/futures/_base.py", line 619, in result_iterator
[rank0]:     yield _result_or_cancel(fs.pop())
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/concurrent/futures/_base.py", line 317, in _result_or_cancel
[rank0]:     return fut.result(timeout)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/concurrent/futures/_base.py", line 456, in result
[rank0]:     return self.__get_result()
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
[rank0]:     raise self._exception
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/concurrent/futures/thread.py", line 58, in run
[rank0]:     result = self.fn(*self.args, **self.kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py", line 306, in _inner_hf_hub_download
[rank0]:     return hf_hub_download(
[rank0]:            ^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
[rank0]:     return _hf_hub_download_to_cache_dir(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1168, in _hf_hub_download_to_cache_dir
[rank0]:     _download_to_tmp_and_move(
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1720, in _download_to_tmp_and_move
[rank0]:     xet_get(
[rank0]:   File "/root/anaconda3/envs/585/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 626, in xet_get
[rank0]:     download_files(
[rank0]: RuntimeError: Data processing error: CAS service error : IO Error: Disk quota exceeded (os error 122)
[rank0]:[W1211 06:37:34.021632690 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
